{
    "data": [
        {
            "BD": 0.866,
            "mAP": 0.795,
            "BC": 0.854,
            "HC": 0.761,
            "CC": 0.571,
            "TeamNames": "pca_lab",
            "LV": 0.794,
            "Harbor": 0.841,
            "Plane": 0.883,
            "RA": 0.774,
            "TC": 0.909,
            "TeamMembers": "Chengzheng Li, Chunyan Xu, Zhen Cui, Dan Wang, Tong Zhang, Jian Yang",
            "description": "Our method is FPN[1] based Faster R-CNN[2], we add rotate branch to RCNN head to do regression of horizontal and rotate boxes parallelly. Firstly, we enhance the FPN structure using deformable convolutions[3]. Secondly, we add semantic segmentation branch to RPN head to generate box-wise segmentation prediction and semantic feature of the whole image as [4] did. Thirdly, we utilize multi-stage FPN feature, semantic feature as well as the original image to do feature fusion, and feed the fused feature into RCNN head to do the final prediction. In both training and testing, we split the original images into 1024*1024 patches with 512 gap, and use DOTA_devkit to merge results when testing. For training, we take both training set and validation set. In addition, we use multi-scale training&testing, image flip&rotation. We use three models ResNet101, ResNeXt101 and ResNet101 mdcn version as backbone to do model ensemble. The final result is ensemble version. [1] Lin T Y, Doll√°r P, Girshick R B, et al. Feature Pyramid Networks for Object Detection[C]//CVPR. 2017, 1(2): 4. [2] Ren S, He K, Girshick R, et al. Faster r-cnn: Towards real-time object detection with region proposal networks[C]//Advances in neural information processing systems. 2015: 91-99. [3] Zhu X, Hu H, Lin S, et al. Deformable convnets v2: More deformable, better results[J]. arXiv preprint arXiv:1811.11168, 2018. [4]. Chen K, Pang J, Wang J, et al. Hybrid task cascade for instance segmentation[J]. arXiv preprint arXiv:1901.07518, 2019.",
            "GTF": 0.798,
            "Institute": "Nanjing University of Science and Technology",
            "date": "2019-04-15 23:12:12",
            "Bridge": 0.657,
            "SP": 0.811,
            "SV": 0.746,
            "ST": 0.842,
            "SBF": 0.739,
            "created_date": "2019-04-15 23:12:12",
            "Ship": 0.881
        },
        {
            "BD": 0.856,
            "mAP": 0.792,
            "BC": 0.859,
            "HC": 0.765,
            "CC": 0.571,
            "TeamNames": "USTC-NELSLIP",
            "LV": 0.811,
            "Harbor": 0.817,
            "Plane": 0.893,
            "RA": 0.763,
            "TC": 0.908,
            "TeamMembers": "Yixing Zhu, Xueqing Wu, Jiaming Wang, Jun Du",
            "description": "Our method is based on FPN (https://arxiv.org/abs/1612.03144), and we augment the network as PANet does (https://arxiv.org/abs/1803.01534). Anchor-free (https://arxiv.org/ftp/arxiv/papers/1804/1804.09003.pdf) is adopted in RPN stage, and we directly regress oriented bounding box in this stage. Then we propose the oriented bounding box in R-CNN with Rotated RoIAlign (https://arxiv.org/abs/1703.06870). We adopt Cascade R-CNN (https://arxiv.org/abs/1712.00726) with two steps. The proposed bounding boxes are both oriented in these two steps, but different IoU calculation methods are used. In the first stage, the IoU is not related to length of target box, we intercept part of target box's long side to get the largest IoU between proposed box and the intercepted target box. In this way, some long boxes will also have corresponding positive samples. Traditional IoU calculation method is used in the second R-CNN. We calculate the minimum bounding box including oriented bounding box as horizonal bounding box. Class balance resampling, image rotation, multi-scale training & testing and model assembling are used for better performance. When model assembling, three models are used whose backbone is ResNeXt-101(32x4). Finally, we combine training set with validation set for training. Our code is implemented based on mmdetection (https://github.com/open-mmlab/mmdetection/). ",
            "GTF": 0.809,
            "Institute": "University of Science and Technology of China",
            "date": "2019-04-15 23:33:51",
            "Bridge": 0.596,
            "SP": 0.818,
            "SV": 0.752,
            "ST": 0.857,
            "SBF": 0.695,
            "created_date": "2019-04-15 23:33:51",
            "Ship": 0.896
        },
        {
            "BD": 0.856,
            "mAP": 0.784,
            "BC": 0.857,
            "HC": 0.746,
            "CC": 0.44,
            "TeamNames": "AICyber",
            "LV": 0.815,
            "Harbor": 0.829,
            "Plane": 0.892,
            "RA": 0.763,
            "TC": 0.908,
            "TeamMembers": "Yang Xue; Yang Jirui; Wang Yashan; Zhang Yue; Sun Xian; Fu Kun",
            "description": "<a href='leaderboards/Description/AICyber DOAI2019 Task1 & Task2 Description.pdf'>Description of method</a>",
            "GTF": 0.741,
            "Institute": "IECAS",
            "date": "2019-04-15 21:20:27",
            "Bridge": 0.644,
            "SP": 0.829,
            "SV": 0.774,
            "ST": 0.86,
            "SBF": 0.698,
            "created_date": "2019-04-15 21:20:27",
            "Ship": 0.896
        },
        {
            "BD": 0.839,
            "mAP": 0.764,
            "BC": 0.855,
            "HC": 0.698,
            "CC": 0.502,
            "TeamNames": "wonderwall",
            "LV": 0.749,
            "Harbor": 0.781,
            "Plane": 0.877,
            "RA": 0.74,
            "TC": 0.909,
            "TeamMembers": "Qi Dang, Shaoxiong Li, Ting Chen, Shouping Shan, Xianmin Li, Siyu Wang, Lin Chen",
            "description": "We trained two object detection models for this challenge, which are Faster RCNN (https://arxiv.org/abs/1506.01497) and Cascade RCNN (https://arxiv.org/abs/1712.00726), and the final result is fused by voting strategy. Faster RCNN is implemented by Detectron (https://github.com/facebookresearch/Detectron). Cascade RCNN is implemented by mmdetection (https://github.com/open-mmlab/mmdetection). For data preparation, we first cropped aerial images into 1024*1024 size by the official DOTA_devkit (https://github.com/CAPTAIN-WHU/DOTA_devkit) in multi scales, then the images are augmented by flipping, rotation, random cropping, grayscale, oversampling. The training skills we mainly adopted are multi-scale training and testing, model assembling, soft-NMS (Non-Maximum Suppression, https://arxiv.org/abs/1704.04503), warm-up, specific anchor ratio, RoIAlign, FPN (Feature Pyramid Network, https://arxiv.org/abs/1506.01497). We used pre-trained ResNet-50, ResNeXt-101 as backbone network respectively. At last, the training data and validation data are combined for training the final model.",
            "GTF": 0.776,
            "Institute": "Xidian University & Huawei Cloud",
            "date": "2019-04-15 23:51:35",
            "Bridge": 0.547,
            "SP": 0.809,
            "SV": 0.743,
            "ST": 0.844,
            "SBF": 0.662,
            "created_date": "2019-04-15 23:51:35",
            "Ship": 0.89
        },
        {
            "BD": 0.85,
            "mAP": 0.762,
            "BC": 0.854,
            "HC": 0.741,
            "CC": 0.391,
            "TeamNames": "czh",
            "LV": 0.803,
            "Harbor": 0.811,
            "Plane": 0.88,
            "RA": 0.7,
            "TC": 0.908,
            "TeamMembers": "Zhonghan Chang",
            "description": "Our teamname is NIST-AITeam mebers are Kun Fu, Xian Sun, Zhonghan Chang, Yue Zhang, Zhuo Chen, Yingchao Feng, Tengfei Zhang and Peijin Wang. We come from School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences. And we also come from Key Laboratory of Network Information System Technology, Institute of Electronics, Chinese Academy of Sciences. <a href='leaderboards/Description/DOAI2019-CZH.pdf'>Description of method</a>",
            "GTF": 0.735,
            "Institute": "IECAS",
            "date": "2019-04-11 04:55:02",
            "Bridge": 0.644,
            "SP": 0.806,
            "SV": 0.727,
            "ST": 0.836,
            "SBF": 0.629,
            "created_date": "2019-04-11 04:55:02",
            "Ship": 0.884
        },
        {
            "BD": 0.819,
            "mAP": 0.737,
            "BC": 0.799,
            "HC": 0.698,
            "CC": 0.39,
            "TeamNames": "gwfemma",
            "LV": 0.779,
            "Harbor": 0.791,
            "Plane": 0.8,
            "RA": 0.717,
            "TC": 0.908,
            "TeamMembers": "guowei",
            "description": "<a href='leaderboards/Description/DOAI2019CONTEST-METHOD.pdf'>Description of method</a>",
            "GTF": 0.703,
            "Institute": "Chongqing university",
            "date": "2019-04-15 07:58:42",
            "Bridge": 0.585,
            "SP": 0.786,
            "SV": 0.72,
            "ST": 0.809,
            "SBF": 0.609,
            "created_date": "2019-04-15 07:58:42",
            "Ship": 0.881
        },
        {
            "BD": 0.842,
            "mAP": 0.734,
            "BC": 0.816,
            "HC": 0.517,
            "CC": 0.34,
            "TeamNames": "Kakao Brain",
            "LV": 0.793,
            "Harbor": 0.827,
            "Plane": 0.801,
            "RA": 0.759,
            "TC": 0.908,
            "TeamMembers": "Jonghyuk Park, Jihoon Lee, Taegoo Kim, Ildoo Kim, Woonhyuk Baek, Sungbin Lim",
            "description": "<a href='leaderboards/Description/DOAI 2019 Submission.pdf'>Description of method</a>",
            "GTF": 0.795,
            "Institute": "Kakao Brain",
            "date": "2019-04-15 23:50:09",
            "Bridge": 0.495,
            "SP": 0.774,
            "SV": 0.747,
            "ST": 0.834,
            "SBF": 0.617,
            "created_date": "2019-04-15 23:50:09",
            "Ship": 0.886
        },
        {
            "BD": 0.847,
            "mAP": 0.734,
            "BC": 0.835,
            "HC": 0.591,
            "CC": 0.43,
            "TeamNames": "peijin",
            "LV": 0.808,
            "Harbor": 0.777,
            "Plane": 0.81,
            "RA": 0.722,
            "TC": 0.909,
            "TeamMembers": "peijin",
            "description": "My method is a Feature-Merged Single Shot Detector With An Area-Weighted Loss based on SSD. We propose a Feature-merged network to propagate the semantic information in feature maps effectively. This network employs Atrous Spatial Feature Pyramid module to capture multi-scale context by using feature pyramid and multiple atrous rates, which handles the problem of detecting multi-scale objects, with image-level feature encoding global information. Furthermore, we find it impractical that small objects and large objects are treated fairly to be classified and located in training. We think the smaller objects are, the larger weights in loss are. Inspired by the thought, we propose an area-weighted loss to replace the original loss in detection networks.",
            "GTF": 0.697,
            "Institute": "UCAS",
            "date": "2019-04-11 19:20:33",
            "Bridge": 0.607,
            "SP": 0.77,
            "SV": 0.675,
            "ST": 0.782,
            "SBF": 0.595,
            "created_date": "2019-04-11 19:20:33",
            "Ship": 0.892
        },
        {
            "BD": 0.83,
            "mAP": 0.731,
            "BC": 0.829,
            "HC": 0.611,
            "CC": 0.299,
            "TeamNames": "GoodLuck",
            "LV": 0.795,
            "Harbor": 0.834,
            "Plane": 0.884,
            "RA": 0.637,
            "TC": 0.907,
            "TeamMembers": "WangYingMing, WangDong, LuHuChuan",
            "description": "Our method is based on a two-stage framework.\r\nWe use the convlution layes of ResNet-50 as backbone.\r\nThe feature pyramid network (FPN) which is beneficial for scale variation of objects follows the backbone.\r\nThen the RPN network generates regions of interest (ROI), and RoI-align laye pools the features of RoI.\r\nFinally, the features of RoI feed into the head network to train with bounding box regression, category classification and mask segmentation tasks.\r\nAnd we develop a training and testing mechanism to deal with the large-size images. \r\nWe split the large size images into smaller pathches. \r\nWe also generate fake patches to train together and remove the influence of the divided objects.\r\nTo avoid the problems of predicting the angles of rotated bounding boxes, we predict the rotated bounding boxes by a segmentation task. \r\nThe result ensemble by multi-models and multi-scales.",
            "GTF": 0.724,
            "Institute": "Dalian University of Technology",
            "date": "2019-04-15 21:56:28",
            "Bridge": 0.604,
            "SP": 0.801,
            "SV": 0.694,
            "ST": 0.847,
            "SBF": 0.517,
            "created_date": "2019-04-15 21:56:28",
            "Ship": 0.888
        },
        {
            "BD": 0.819,
            "mAP": 0.731,
            "BC": 0.777,
            "HC": 0.682,
            "CC": 0.319,
            "TeamNames": "CVEO_WHU",
            "LV": 0.768,
            "Harbor": 0.814,
            "Plane": 0.809,
            "RA": 0.722,
            "TC": 0.909,
            "TeamMembers": "Chen Guanzhou, Zhu Kun, Tan Xiaoliang, Zhang Lifei, Liao Puyun",
            "description": "<a href='leaderboards/Description/CVEO-WHU(DOAI2019).pdf'>Description of method</a>",
            "GTF": 0.765,
            "Institute": "Wuhan University",
            "date": "2019-04-07 19:40:25",
            "Bridge": 0.59,
            "SP": 0.747,
            "SV": 0.658,
            "ST": 0.779,
            "SBF": 0.651,
            "created_date": "2019-04-07 19:40:25",
            "Ship": 0.888
        },
        {
            "BD": 0.856,
            "mAP": 0.726,
            "BC": 0.857,
            "HC": 0.746,
            "CC": 0.438,
            "TeamNames": "CSULQQ",
            "LV": 0.813,
            "Harbor": 0.827,
            "Plane": 0.892,
            "RA": 0.763,
            "TC": 0.0,
            "TeamMembers": "Liu Qingqing",
            "description": "<a href='leaderboards/Description/DOTA--METHOD DISCRIPTION.pdf'>Description of method</a>",
            "GTF": 0.741,
            "Institute": "CSU",
            "date": "2019-04-13 23:04:25",
            "Bridge": 0.644,
            "SP": 0.829,
            "SV": 0.77,
            "ST": 0.854,
            "SBF": 0.687,
            "created_date": "2019-04-13 23:04:25",
            "Ship": 0.896
        },
        {
            "BD": 0.832,
            "mAP": 0.711,
            "BC": 0.781,
            "HC": 0.588,
            "CC": 0.384,
            "TeamNames": "BESTORE",
            "LV": 0.756,
            "Harbor": 0.85,
            "Plane": 0.806,
            "RA": 0.713,
            "TC": 0.908,
            "TeamMembers": "zhuozheng",
            "description": "Our basic model is cascade RCNN[1] with dcn[2]-fpn[3]-ResNet[4]-101 as backbone.\r\nThe multi-scale training and test are used in this solution.\r\n\r\n[1]Cai Z, Vasconcelos N. Cascade r-cnn: Delving into high quality object detection[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 6154-6162.\r\n[2]Dai J, Qi H, Xiong Y, et al. Deformable convolutional networks[C]//Proceedings of the IEEE international conference on computer vision. 2017: 764-773.\r\n[3]Lin T Y, Doll\u00e1r P, Girshick R, et al. Feature pyramid networks for object detection[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017: 2117-2125.\r\n[4]He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778.",
            "GTF": 0.722,
            "Institute": "wuhan university",
            "date": "2019-04-13 02:39:32",
            "Bridge": 0.589,
            "SP": 0.767,
            "SV": 0.51,
            "ST": 0.773,
            "SBF": 0.509,
            "created_date": "2019-04-13 02:39:32",
            "Ship": 0.891
        },
        {
            "BD": 0.791,
            "mAP": 0.711,
            "BC": 0.766,
            "HC": 0.647,
            "CC": 0.364,
            "TeamNames": "Gwfemma",
            "LV": 0.758,
            "Harbor": 0.782,
            "Plane": 0.798,
            "RA": 0.702,
            "TC": 0.907,
            "TeamMembers": "guowei",
            "description": "<a href='leaderboards/Description/DOAI2019CONTEST-METHOD.pdf'>Description of method</a>",
            "GTF": 0.673,
            "Institute": "Chongqing University",
            "date": "2019-04-01 05:35:52",
            "Bridge": 0.585,
            "SP": 0.764,
            "SV": 0.649,
            "ST": 0.809,
            "SBF": 0.505,
            "created_date": "2019-04-01 05:35:52",
            "Ship": 0.877
        },
        {
            "BD": 0.812,
            "mAP": 0.675,
            "BC": 0.737,
            "HC": 0.551,
            "CC": 0.195,
            "TeamNames": "Inception",
            "LV": 0.707,
            "Harbor": 0.737,
            "Plane": 0.794,
            "RA": 0.719,
            "TC": 0.907,
            "TeamMembers": "Inception",
            "description":"<a href='leaderboards/Description/DOTAV1.5_method_description.pdf'>Description of method</a>",
            "GTF": 0.635,
            "Institute": "School of Articifical Intelligence",
            "date": "2019-04-15 19:47:35",
            "Bridge": 0.545,
            "SP": 0.755,
            "SV": 0.646,
            "ST": 0.743,
            "SBF": 0.458,
            "created_date": "2019-04-15 19:47:35",
            "Ship": 0.861
        },
        {
            "BD": 0.735,
            "mAP": 0.615,
            "BC": 0.701,
            "HC": 0.465,
            "CC": 0.177,
            "TeamNames": "Pengfei",
            "LV": 0.577,
            "Harbor": 0.736,
            "Plane": 0.692,
            "RA": 0.666,
            "TC": 0.884,
            "TeamMembers": "Pengfei",
            "description": "1. In order to detect object in Aerial Images, and the detection result is shown by the oriented bounding box. we transfer the oriented bounding box to mask annotation. each instance's location is annotated by a quadrilateral bounding boxes, which can be denoted as 'x1, y1, x2, y2, x3, y3, x4, y4' where (xi, yi) denotes the positions of the oriented bounding boxes' vertices in the image. Through these information, we can work out the horizontal bounding box coordinates whose top left corner is [xmin, ymin] and bottom right corner is [xmax, ymax]. xmin and ymin are the minimum among x1, x2, x3, x4 and y1, y2, y3, y4, separately. Similarly, xmax and ymax are the maximum. \n 2. Considering the mainstream target detection algorithm, we choose the Mask R-CNN as our baseline. The backbone is ResNeXt-152 and no other tricks are used in test. The oriented bounding box derived for instance segmentation. The result is about 0.5896. Later, we think Cascade architecture can bring better result. So, the V2(Cascade Mask R-CNN) improve result by 2 points. \n 3. Due to the limited of time and GPU, some idea have to been stopped. The Dataset is so massive that training time is so long. Later, if we have enough time, Cascade Mask R-CNN will  be update to V3(Hybrid Cascade Task).",
            "GTF": 0.58,
            "Institute": "Harbin Institute of Technology",
            "date": "2019-04-14 05:26:53",
            "Bridge": 0.475,
            "SP": 0.676,
            "SV": 0.592,
            "ST": 0.681,
            "SBF": 0.421,
            "created_date": "2019-04-14 05:26:53",
            "Ship": 0.786
        },
        {
            "BD": 0.717,
            "mAP": 0.592,
            "BC": 0.764,
            "HC": 0.467,
            "CC": 0.256,
            "TeamNames": "Adamdad",
            "LV": 0.599,
            "Harbor": 0.631,
            "Plane": 0.786,
            "RA": 0.557,
            "TC": 0.905,
            "TeamMembers": "Xingyi Yang",
            "description": "<a href='leaderboards/Description/DOAI summary2.0.pdf'>Description of method</a>",
            "GTF": 0.608,
            "Institute": "Southeast University",
            "date": "2019-04-09 19:44:35",
            "Bridge": 0.327,
            "SP": 0.627,
            "SV": 0.374,
            "ST": 0.628,
            "SBF": 0.453,
            "created_date": "2019-04-09 19:44:35",
            "Ship": 0.78
        },
        {
            "BD": 0.687,
            "mAP": 0.569,
            "BC": 0.764,
            "HC": 0.272,
            "CC": 0.263,
            "TeamNames": "caiest",
            "LV": 0.687,
            "Harbor": 0.644,
            "Plane": 0.786,
            "RA": 0.394,
            "TC": 0.904,
            "TeamMembers": "Hu yining, Yang Xingyi, Wang Hao, Zhu Yanqing, Zhu Hao, Xu Xiao, Wang Zheng",
            "description": "<a href='leaderboards/Description/DOAI summary2.0.pdf'>Description of method</a>",
            "GTF": 0.49,
            "Institute": "School of Cyber Science and Enginnering, Southeast University",
            "date": "2019-04-15 11:09:59",
            "Bridge": 0.427,
            "SP": 0.617,
            "SV": 0.489,
            "ST": 0.603,
            "SBF": 0.378,
            "created_date": "2019-04-15 11:09:59",
            "Ship": 0.702
        },
        {
            "BD": 0.675,
            "mAP": 0.556,
            "BC": 0.712,
            "HC": 0.475,
            "CC": 0.102,
            "TeamNames": "Adoreeeee",
            "LV": 0.656,
            "Harbor": 0.617,
            "Plane": 0.79,
            "RA": 0.0,
            "TC": 0.868,
            "TeamMembers": "Zhuo Wang, Zhan Su, Lingyun Wu, Haonan Qin, Jie Lei, Weiying Xie",
            "description": "The test results we submit are generated with our detection network which use a novel one - stage detection structure and achieve good results in remote sensing image detection. We took the first-order hourglass network as our backbone network, and added four detectors to the backbone network to detect objects of different scales, and superimposed the features of different scales on the channels, that is, features of different scales were fused. In addition, we also use the feature enhancement network to prevent the feature information of small objects from gradually disappearing with the increase of network depth. Our method effectively improves the accuracy of target detection, but the detection speed is still in the leading level.The results of this submission is based on the model we have trained for 19,700 epochs.",
            "GTF": 0.293,
            "Institute": "XiDian University",
            "date": "2019-04-01 05:35:52",
            "Bridge": 0.484,
            "SP": 0.634,
            "SV": 0.719,
            "ST": 0.742,
            "SBF": 0.294,
            "created_date": "2019-04-01 05:35:52",
            "Ship": 0.842
        },
        {
            "BD": 0.542,
            "mAP": 0.474,
            "BC": 0.578,
            "HC": 0.424,
            "CC": 0.123,
            "TeamNames": "huangqiuyu",
            "LV": 0.528,
            "Harbor": 0.51,
            "Plane": 0.585,
            "RA": 0.451,
            "TC": 0.683,
            "TeamMembers": "Qiuyu Huang, Feipeng Zhou",
            "description": "This is the first submission, using the most basic model of object detection. The network structure is the fater rcnn res101 fpn. The data is preprocessed according to the official script provided, then be entered into the fater rcnn res101 fpn neural network to train, and finally the test is performed. The difference between DOTA version 1.0 and version 1.5 is relatively large, so a minor adjustment is made here, but for the accuracy this adjustment is not significant.",
            "GTF": 0.411,
            "Institute": "Beihang university",
            "date": "2019-04-14 01:21:36",
            "Bridge": 0.352,
            "SP": 0.527,
            "SV": 0.45,
            "ST": 0.523,
            "SBF": 0.264,
            "created_date": "2019-04-14 01:21:36",
            "Ship": 0.63
        },
        {
            "BD": 0.684,
            "mAP": 0.473,
            "BC": 0.669,
            "HC": 0.196,
            "CC": 0.16,
            "TeamNames": "lux",
            "LV": 0.317,
            "Harbor": 0.436,
            "Plane": 0.63,
            "RA": 0.449,
            "TC": 0.904,
            "TeamMembers": "Lu kaixuan, Zan luyang",
            "description": "No Discription",
            "GTF": 0.544,
            "Institute": "AirCas",
            "date": "2019-04-14 07:37:54",
            "Bridge": 0.22,
            "SP": 0.679,
            "SV": 0.224,
            "ST": 0.482,
            "SBF": 0.352,
            "created_date": "2019-04-14 07:37:54",
            "Ship": 0.616
        },        
        {
            "BD": 0.477,
            "mAP": 0.456,
            "BC": 0.488,
            "HC": 0.26,
            "CC": 0.216,
            "TeamNames": "ADAMYXY",
            "LV": 0.572,
            "Harbor": 0.494,
            "Plane": 0.642,
            "RA": 0.349,
            "TC": 0.709,
            "TeamMembers": "Xingyi Yang",
            "description": "We design a one-stage quadrilateral detector based on YOLOv3.In addition to the horizon bounding box regression, we stimulate the method of Textbox++ for predict the relative position of four vertexes to the HBB using logistic regression. The geometric constraint loss is also added to make sure our prediction results are more similar to standard markings. The focal loss and class-aware sampling is used for the severe sample imbalance in the DOTA annotation. In the training stage, rotation data-augmentation and multi-scale training is adapted to enhance the robustness of our model. The post-processing includes quadrilateral NMS and vertexes mapping. With multi-scale testing, we can give accurate detection result with 5fps for 1024*1024 input images. The backend is Resnet101-FPN,VGG-FPN and ResNeXt101-FPN trained on imagenet.",
            "GTF": 0.325,
            "Institute": "Southeast University",
            "date": "2019-04-15 10:53:23",
            "Bridge": 0.389,
            "SP": 0.54,
            "SV": 0.439,
            "ST": 0.558,
            "SBF": 0.191,
            "created_date": "2019-04-15 10:53:23",
            "Ship": 0.647
        },
        {
            "BD": 0.091,
            "mAP": 0.126,
            "BC": 0.045,
            "HC": 0.0,
            "CC": 0.0,
            "TeamNames": "SerenaHe",
            "LV": 0.254,
            "Harbor": 0.014,
            "Plane": 0.218,
            "RA": 0.091,
            "TC": 0.036,
            "TeamMembers": "Shiyin He, Yuan Hua, Xuan Peng",
            "description": "this is a submission with result of an original mask-rcnn model. the train set and val set are splited into 800*800 with 200 overlap and they are transformed into a coco dataset format. There might be some empty files--baseball diamond,basketball court,container crane, roundabout and soccer ball field. these are replaced with result from the last try. The model is trained under pytorch. Data of horizontal bounding boxes is not used however, in this task, label in task1 is used as training data as the mask and horizontal bounding boxes are extracted from masks.The result was reached with 50 epoches and 1000 images per step. there are some question about ResultMerge because too many objects in some txt.Let\u2019s see how this network work.",
            "GTF": 0.0,
            "Institute": "Shanghai Jiao Tong University",
            "date": "2019-04-15 09:18:35",
            "Bridge": 0.115,
            "SP": 0.281,
            "SV": 0.232,
            "ST": 0.232,
            "SBF": 0.091,
            "created_date": "2019-04-15 09:18:35",
            "Ship": 0.313
        }
        
    ]
}
