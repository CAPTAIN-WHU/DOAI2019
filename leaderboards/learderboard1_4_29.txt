{
    "data": [
        {
            "BD": 0.853,
            "mAP": 0.783,
            "BC": 0.859,
            "HC": 0.778,
            "CC": 0.573,
            "TeamNames": "USTC-NELSLIP",
            "LV": 0.813,
            "Harbor": 0.763,
            "Plane": 0.892,
            "RA": 0.767,
            "TC": 0.908,
            "TeamMembers": "Yixing Zhu, Xueqing Wu, Jiaming Wang, Jun Du",
            "description": "Our method is based on FPN (https://arxiv.org/abs/1612.03144), and we augment the network as PANet does (https://arxiv.org/abs/1803.01534). Anchor-free (https://arxiv.org/ftp/arxiv/papers/1804/1804.09003.pdf) is adopted in RPN stage, and we directly regress oriented bounding box in this stage. Then we propose the oriented bounding box in R-CNN with Rotated RoIAlign (https://arxiv.org/abs/1703.06870). We adopt Cascade R-CNN (https://arxiv.org/abs/1712.00726) with two steps. The proposed bounding boxes are both oriented in these two steps, but different IoU calculation methods are used. In the first stage, the IoU is not related to length of target box, we intercept part of target box's long side to get the largest IoU between proposed box and the intercepted target box. In this way, some long boxes will also have corresponding positive samples. Traditional IoU calculation method is used in the second R-CNN. Class balance resampling, image rotation, multi-scale training & testing and model assembling are used for better performance. When model assembling, three models are used whose backbone is ResNeXt-101(32x4). Finally, we combine training set with validation set for training. Our code is implemented based on mmdetection (https://github.com/open-mmlab/mmdetection/). ",
            "GTF": 0.809,
            "Institute": "University of Science and Technology of China",
            "date": "2019-04-15 23:45:59",
            "Bridge": 0.573,
            "SP": 0.76,
            "SV": 0.739,
            "ST": 0.856,
            "SBF": 0.695,
            "created_date": "2019-04-15 23:45:59",
            "Ship": 0.895
        },
        {
            "BD": 0.864,
            "mAP": 0.766,
            "BC": 0.853,
            "HC": 0.695,
            "CC": 0.496,
            "TeamNames": "pca_lab",
            "LV": 0.756,
            "Harbor": 0.764,
            "Plane": 0.882,
            "RA": 0.775,
            "TC": 0.909,
            "TeamMembers": "Chengzheng Li, Chunyan Xu, Zhen Cui, Dan Wang, Tong Zhang, Jian Yang",
            "description": "Our method is FPN[1] based Faster R-CNN[2], we add rotate branch to RCNN head to do regression of horizontal and rotate boxes parallelly. Firstly, we enhance the FPN structure using deformable convolutions[3]. Secondly, we add semantic segmentation branch to RPN head to generate box-wise segmentation prediction and semantic feature of the whole image as [4] did. Thirdly, we utilize multi-stage FPN feature, semantic feature as well as the original image to do feature fusion, and feed the fused feature into RCNN head to do the final prediction. In both training and testing, we split the original images into 1024*1024 patches with 512 gap, and use DOTA_devkit to merge results when testing. For training, we take both training set and validation set. In addition, we use multi-scale training&testing, image flip&rotation. We use three models ResNet101, ResNeXt101 and ResNet101 mdcn version as backbone to do model ensemble. The final result is ensemble version. [1] Lin T Y, Doll√°r P, Girshick R B, et al. Feature Pyramid Networks for Object Detection[C]//CVPR. 2017, 1(2): 4. [2] Ren S, He K, Girshick R, et al. Faster r-cnn: Towards real-time object detection with region proposal networks[C]//Advances in neural information processing systems. 2015: 91-99. [3] Zhu X, Hu H, Lin S, et al. Deformable convnets v2: More deformable, better results[J]. arXiv preprint arXiv:1811.11168, 2018. [4]. Chen K, Pang J, Wang J, et al. Hybrid task cascade for instance segmentation[J]. arXiv preprint arXiv:1901.07518, 2019.",
            "GTF": 0.8,
            "Institute": "Nanjing University of Science and Technology",
            "date": "2019-04-15 23:11:01",
            "Bridge": 0.594,
            "SP": 0.737,
            "SV": 0.681,
            "ST": 0.841,
            "SBF": 0.738,
            "created_date": "2019-04-15 23:11:01",
            "Ship": 0.872
        },
        {
            "BD": 0.832,
            "mAP": 0.757,
            "BC": 0.844,
            "HC": 0.734,
            "CC": 0.421,
            "TeamNames": "czh",
            "LV": 0.803,
            "Harbor": 0.742,
            "Plane": 0.89,
            "RA": 0.753,
            "TC": 0.908,
            "TeamMembers": "Zhonghan Chang",
            "description": "Our teamname is NIST-AITeam mebers are Kun Fu, Xian Sun, Zhonghan Chang, Yue Zhang, Zhuo Chen, Yingchao Feng, Tengfei Zhang and Peijin Wang. We come from School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences. And we also come from Key Laboratory of Network Information System Technology, Institute of Electronics, Chinese Academy of Sciences.<a href='leaderboards/Description/DOAI2019-CZH.pdf'>Description of method</a>",
            "GTF": 0.738,
            "Institute": "IECAS",
            "date": "2019-04-11 22:58:24",
            "Bridge": 0.545,
            "SP": 0.744,
            "SV": 0.726,
            "ST": 0.85,
            "SBF": 0.687,
            "created_date": "2019-04-11 22:58:24",
            "Ship": 0.893
        },
        {
            "BD": 0.854,
            "mAP": 0.747,
            "BC": 0.863,
            "HC": 0.734,
            "CC": 0.379,
            "TeamNames": "AICyber",
            "LV": 0.727,
            "Harbor": 0.741,
            "Plane": 0.884,
            "RA": 0.76,
            "TC": 0.909,
            "TeamMembers": "Yang Xue; Yang Jirui; Wang Yashan; Zhang Yue; Sun Xian; Fu Kun",
            "description": "<a href='leaderboards/Description/AICyber DOAI2019 Task1 & Task2 Description.pdf'>Description of method</a>",
            "GTF": 0.744,
            "Institute": "IECAS",
            "date": "2019-04-13 22:46:26",
            "Bridge": 0.567,
            "SP": 0.729,
            "SV": 0.639,
            "ST": 0.85,
            "SBF": 0.689,
            "created_date": "2019-04-13 22:46:26",
            "Ship": 0.879
        },
        {
            "BD": 0.836,
            "mAP": 0.723,
            "BC": 0.846,
            "HC": 0.688,
            "CC": 0.225,
            "TeamNames": "CSULQQ",
            "LV": 0.71,
            "Harbor": 0.674,
            "Plane": 0.878,
            "RA": 0.755,
            "TC": 0.908,
            "TeamMembers": "Liu Qingqing",
            "description": "<a href='leaderboards/Description/DOTA--METHOD DISCRIPTION.pdf'>Description of method</a>",
            "GTF": 0.744,
            "Institute": "CSU",
            "date": "2019-04-11 19:40:54",
            "Bridge": 0.567,
            "SP": 0.712,
            "SV": 0.632,
            "ST": 0.84,
            "SBF": 0.678,
            "created_date": "2019-04-11 19:40:54",
            "Ship": 0.878
        },
        {
            "BD": 0.836,
            "mAP": 0.716,
            "BC": 0.792,
            "HC": 0.598,
            "CC": 0.395,
            "TeamNames": "peijin",
            "LV": 0.764,
            "Harbor": 0.741,
            "Plane": 0.809,
            "RA": 0.748,
            "TC": 0.909,
            "TeamMembers": "peijin",
            "description": "My method is a Feature-Merged Single Shot Detector With An Area-Weighted Loss based on SSD. We propose a Feature-merged network to propagate the semantic information in feature maps effectively. This network employs Atrous Spatial Feature Pyramid module to capture multi-scale context by using feature pyramid and multiple atrous rates, which handles the problem of detecting multi-scale objects, with image-level feature encoding global information. Furthermore, we find it impractical that small objects and large objects are treated fairly to be classified and located in training. We think the smaller objects are, the larger weights in loss are. Inspired by the thought, we propose an area-weighted loss to replace the original loss in detection networks.",
            "GTF": 0.707,
            "Institute": "UCAS",
            "date": "2019-04-11 19:18:54",
            "Bridge": 0.551,
            "SP": 0.749,
            "SV": 0.599,
            "ST": 0.783,
            "SBF": 0.591,
            "created_date": "2019-04-11 19:18:54",
            "Ship": 0.883
        },
        {
            "BD": 0.825,
            "mAP": 0.707,
            "BC": 0.828,
            "HC": 0.609,
            "CC": 0.256,
            "TeamNames": "GoodLuck",
            "LV": 0.767,
            "Harbor": 0.767,
            "Plane": 0.882,
            "RA": 0.636,
            "TC": 0.907,
            "TeamMembers": "WangYingMing, WangDong, LuHuChuan",
            "description": "Our method is based on a two-stage framework.\r\nWe use the convlution layes of ResNet-50 as backbone.\r\nThe feature pyramid network (FPN) which is beneficial for scale variation of objects follows the backbone.\r\nThen the RPN network generates regions of interest (ROI), and RoI-align laye pools the features of RoI.\r\nFinally, the features of RoI feed into the head network to train with bounding box regression, category classification and mask segmentation tasks.\r\nAnd we develop a training and testing mechanism to deal with the large-size images. \r\nWe split the large size images into smaller pathches. \r\nWe also generate fake patches to train together and remove the influence of the divided objects.\r\nTo avoid the problems of predicting the angles of rotated bounding boxes, we predict the rotated bounding boxes by a segmentation task. \r\nThe result ensemble by multi-models and multi-scales.",
            "GTF": 0.735,
            "Institute": "Dalian University of Technology",
            "date": "2019-04-15 22:10:55",
            "Bridge": 0.548,
            "SP": 0.742,
            "SV": 0.596,
            "ST": 0.84,
            "SBF": 0.511,
            "created_date": "2019-04-15 22:10:55",
            "Ship": 0.868
        },
        {
            "BD": 0.728,
            "mAP": 0.609,
            "BC": 0.701,
            "HC": 0.465,
            "CC": 0.154,
            "TeamNames": "Pengfei",
            "LV": 0.666,
            "Harbor": 0.65,
            "Plane": 0.69,
            "RA": 0.667,
            "TC": 0.886,
            "TeamMembers": "Pengfei",
            "description": "1. In order to detect object in Aerial Images, and the detection result is shown by the oriented bounding box. we transfer the oriented bounding box to mask annotation. each instance's location is annotated by a quadrilateral bounding boxes, which can be denoted as 'x1, y1, x2, y2, x3, y3, x4, y4' where (xi, yi) denotes the positions of the oriented bounding boxes' vertices in the image. Through these information, we can work out the horizontal bounding box coordinates whose top left corner is [xmin, ymin] and bottom right corner is [xmax, ymax]. xmin and ymin are the minimum among x1, x2, x3, x4 and y1, y2, y3, y4, separately. Similarly, xmax and ymax are the maximum.\n 2. Considering the mainstream target detection algorithm, we choose the Mask R-CNN as our baseline. The backbone is ResNeXt-152 and no other tricks are used in test. The oriented bounding box derived for instance segmentation. The result is about 0.5896. Later, we think Cascade architecture can bring better result. So, the V2(Cascade Mask R-CNN) improve result by 2 points.\n 3. Due to the limited of time and GPU, some idea have to been stopped. The Dataset is so massive that training time is so long. Later, if we have enough time, Cascade Mask R-CNN will  be update to V3(Hybrid Cascade Task).",
            "GTF": 0.571,
            "Institute": "Harbin Institute of Technology",
            "date": "2019-04-14 05:24:31",
            "Bridge": 0.463,
            "SP": 0.652,
            "SV": 0.564,
            "ST": 0.679,
            "SBF": 0.418,
            "created_date": "2019-04-14 05:24:31",
            "Ship": 0.785
        },
        {
            "BD": 0.748,
            "mAP": 0.574,
            "BC": 0.756,
            "HC": 0.448,
            "CC": 0.28,
            "TeamNames": "SEU_YXY",
            "LV": 0.591,
            "Harbor": 0.534,
            "Plane": 0.796,
            "RA": 0.397,
            "TC": 0.907,
            "TeamMembers": "XixiWang",
            "description": "<a href='leaderboards/Description/DOAI summary2.0.pdf'>Description of method</a>",
            "GTF": 0.564,
            "Institute": "Galaxy Space",
            "date": "2019-04-15 09:20:07",
            "Bridge": 0.371,
            "SP": 0.603,
            "SV": 0.461,
            "ST": 0.649,
            "SBF": 0.388,
            "created_date": "2019-04-15 09:20:07",
            "Ship": 0.693
        },
        {
            "BD": 0.748,
            "mAP": 0.574,
            "BC": 0.756,
            "HC": 0.447,
            "CC": 0.281,
            "TeamNames": "ADAMYXY",
            "LV": 0.591,
            "Harbor": 0.535,
            "Plane": 0.796,
            "RA": 0.397,
            "TC": 0.907,
            "TeamMembers": "Xingyi Yang",
            "description": "We design a one-stage quadrilateral detector based on YOLOv3.In addition to the horizon bounding box regression, we stimulate the method of Textbox++ for predict the relative position of four vertexes to the HBB using logistic regression. The geometric constraint loss is also added to make sure our prediction results are more similar to standard markings. The focal loss and class-aware sampling is used for the severe sample imbalance in the DOTA annotation. In the training stage, rotation data-augmentation and multi-scale training is adapted to enhance the robustness of our model. The post-processing includes quadrilateral NMS and vertexes mapping. With multi-scale testing, we can give accurate detection result with 5fps for 1024*1024 input images. The backend is Resnet101-FPN,VGG-FPN and ResNeXt101-FPN trained on imagenet.",
            "GTF": 0.563,
            "Institute": "Southeast University",
            "date": "2019-04-15 10:35:15",
            "Bridge": 0.364,
            "SP": 0.603,
            "SV": 0.461,
            "ST": 0.648,
            "SBF": 0.389,
            "created_date": "2019-04-15 10:35:15",
            "Ship": 0.694
        },
        {
            "BD": 0.748,
            "mAP": 0.574,
            "BC": 0.756,
            "HC": 0.447,
            "CC": 0.281,
            "TeamNames": "caiest",
            "LV": 0.591,
            "Harbor": 0.535,
            "Plane": 0.796,
            "RA": 0.397,
            "TC": 0.907,
            "TeamMembers": "Hu yining, Yang Xingyi, Wang Hao, Zhu Yanqing, Zhu Hao, Xu Xiao, Wang Zheng",
            "description": "<a href='leaderboards/Description/DOAI summary2.0.pdf'>Description of method</a>",
            "GTF": 0.563,
            "Institute": "School of Cyber Science and Enginnering, Southeast University",
            "date": "2019-04-15 11:09:05",
            "Bridge": 0.364,
            "SP": 0.603,
            "SV": 0.461,
            "ST": 0.648,
            "SBF": 0.389,
            "created_date": "2019-04-15 11:09:05",
            "Ship": 0.694
        },
        {
            "BD": 0.702,
            "mAP": 0.508,
            "BC": 0.615,
            "HC": 0.475,
            "CC": 0.018,
            "TeamNames": "Nuanyang",
            "LV": 0.428,
            "Harbor": 0.463,
            "Plane": 0.715,
            "RA": 0.591,
            "TC": 0.901,
            "TeamMembers": "Kuncai\uff0cZhang",
            "description": "<a href='leaderboards/Description/DOTA 2019 Challenge Solution.pdf'>Description of method</a>",
            "GTF": 0.568,
            "Institute": "Zhejiang University",
            "date": "2019-04-14 23:36:20",
            "Bridge": 0.336,
            "SP": 0.46,
            "SV": 0.316,
            "ST": 0.6,
            "SBF": 0.36,
            "created_date": "2019-04-14 23:36:20",
            "Ship": 0.577
        },
        {
            "BD": 0.668,
            "mAP": 0.499,
            "BC": 0.756,
            "HC": 0.445,
            "CC": 0.155,
            "TeamNames": "Adamdad",
            "LV": 0.371,
            "Harbor": 0.397,
            "Plane": 0.707,
            "RA": 0.549,
            "TC": 0.898,
            "TeamMembers": "Xingyi Yang",
            "description": "<a href='leaderboards/Description/DOAI summary2.0.pdf'>Description of method</a>",
            "GTF": 0.545,
            "Institute": "Southeast University",
            "date": "2019-04-08 19:30:28",
            "Bridge": 0.207,
            "SP": 0.421,
            "SV": 0.267,
            "ST": 0.62,
            "SBF": 0.429,
            "created_date": "2019-04-08 19:30:28",
            "Ship": 0.557
        }
    ]
}
